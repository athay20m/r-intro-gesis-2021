---
title: "Introduction to R for Data Analysis"
subtitle: "Exploratory Data Analysis"
author: "Johannes Breuer<br />Stefan JÃ¼nger"
date: "2020-08-06"
location: "GESIS Summer School in Survey Methodology"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---
layout: true

```{r setup, include = F}
if (!require(easypackages)) install.packages("easypackages")
library(easypackages)

packages("rmarkdown", "knitr", "kableExtra", "gadenbuie/xaringanExtra", "hadley/emo", "tidyverse", "magrittr", prompt = F)

options(htmltools.dir.version = FALSE)

opts_chunk$set(echo = TRUE, fig.align = "center")

xaringanExtra::use_xaringan_extra(c("tile_view", "clipboard"))
xaringanExtra::use_extra_styles(hover_code_line = TRUE,
                                mute_unhighlighted_code = FALSE)
```

<div class="my-footer">
  <div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
  <div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
  <div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>

<style type="text/css">

pre {
  font-size: 10px
}
</style>

---

## Exploratory Data Analysis (EDA)

After wrangling our data, the next thing we should do is exploring them. In practice, of course, these steps are often done iteratively. Exploratory data analysis can take many shapes and forms. In this session, we will look at the following:

- summary statistics & frequencies
- correlations & cross-tabulations
- checking (joint & grouped) distributions of variables
- checking for missing values and outliers

A key tool for EDA is the use of visualizations which is why we will use some of the visualization techniques discussed in the previous sessions to explore our data.

---

## Data

As using the full dataset can become somewhat unwieldy for the examples in this section, we will create/use a subset of the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* data. We will select a subset of variables on the following:
- demographics
- political orientation
- risk perceptions
- personal measures taken
- trust in people and institutions
- use of media to get Corona-related information

As a repetition and reminder, we will quickly go through a wrangling pipeline for these data in the following.

*Note*: Of course, it is possible to do the whole wrangling in one pipe. However, to check if everything worked it is advisable to break up the pipe into smaller chunks (a nice tool for checking and debugging pipes that also provides an *RStudio* Addin is the package [`ViewPipeSteps`](https://github.com/daranzolin/ViewPipeSteps)). Also, splitting up the wrangling pipe steps allows us to show them on the slides.

---

## Wrangling pipeline: Select & rename 

.small[
```{r load-gesis-panel-data display, eval = F}
gesis_panel_corona <- read_csv2("./data/ZA5667_v1-1-0.csv")
```

```{r load-gesis-panel-data, echo = F, message = F}
gesis_panel_corona <- read_csv2("../../data/ZA5667_v1-1-0.csv")
```

```{r select-rename}
corona_survey <- gesis_panel_corona %>% 
  select(id,
         sex:education_cat,
         choice_of_party,
         left_right = political_orientation,
         risk_self =  hzcy001a,
         risk_surround =  hzcy002a,
         avoid_places =  hzcy006a,
         keep_distance =  hzcy007a,
         wash_hands = hzcy011a,
         stockup_supplies =  hzcy013a,
         reduce_contacts =  hzcy014a,
         wear_mask = hzcy015a,
         trust_rki = hzcy047a,
         trust_government = hzcy048a,
         trust_chancellor = hzcy049a,
         trust_who = hzcy051a,
         trust_scientists = hzcy052a,
         info_national_public_tv = hzcy084a,
         info_national_newspaper = hzcy086a,
         info_local_newspaper = hzcy089a,
         info_facebook = hzcy090a,
         info_other_social_media = hzcy091a)
```
]

---

## Wrangling pipeline: Missing values

If you look at the [codebook](https://dbk.gesis.org/dbksearch/download.asp?id=67378) for the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany*, you will see that some of the variables we have selected have specific values that we should either code as missing values or exclude before we do any analyses (exploratory or otherwise).

.small[
```{r pipeline-missings}
library(naniar)

missings <- c(-111, -99, -77, -33, -22)

corona_survey <- corona_survey %>%
  replace_with_na_all(condition = ~.x %in% missings) %>% 
    replace_with_na(replace = list(choice_of_party = c(97,98),
                                   risk_self = c(97),
                                   risk_surround = c(97),
                                   trust_rki = c(98),
                                   trust_government = c(98),
                                   trust_chancellor = c(98),
                                   trust_who = c(98),
                                   trust_scientists = c(98)))
```
]

---

## Wrangling pipeline: Change variable types, recode values, & compute new variables

.smaller[
```{r var-type-recode}
corona_survey <- corona_survey %>% 
    mutate(sex = recode_factor(sex,
                               `1`= "Male",
                               `2` = "Female"),
           education_cat = recode_factor(education_cat,
                                       `1` = "Low",
                                       `2` = "Medium",
                                       `3`= "High",
                                       .ordered = TRUE),
           age_cat = recode_factor(age_cat,
                                   `1`= "<= 25 years",
                                   `2`= "26 to 30 years",
                                   `3` = "31 to 35 years",
                                   `4` = "36 to 40 years",
                                   `5` = "41 to 45 years",
                                   `6` = "46 to 50 years",
                                   `7` = "51 to 60 years",
                                   `8` = "61 to 65 years",
                                   `9`= "66 to 70 years",
                                   `10` = ">= 71 years",
                                   .ordered = TRUE),
           choice_of_party = recode_factor(choice_of_party,
                                           `1`= "CDU/CSU",
                                           `2`= "SPD",
                                           `3` = "FDP",
                                           `4` = "Linke",
                                           `5` = "Gruene",
                                           `6` = "AfD",
                                           `7` = "Other")
    )
```
]

---

## Wrangling pipeline: Compute new variables

```{r new-vars}
corona_survey <- corona_survey %>%
  mutate(sum_measures = avoid_places + 
           keep_distance + 
           wash_hands + 
           stockup_supplies + 
           reduce_contacts + 
           wear_mask,
         sum_sources = info_national_public_tv + 
           info_national_newspaper + 
           info_local_newspaper + 
           info_facebook + 
           info_other_social_media) %>% 
  rowwise() %>% 
  mutate(mean_trust = mean(c(trust_rki, 
                             trust_government, 
                             trust_chancellor, 
                             trust_who, 
                             trust_scientists),
                           na.rm = TRUE)) %>% 
  ungroup()
```

---

## Explore your data: First look

To get a first impression of the dataset you can use some of the functions that we discussed in the session on *Data Wrangling*, such as `dim()`, `head()`, or `str()` from `base R`, `glimpse()` from `dplyr`, or `View()`.

While looking at the the full dataset can give us a general understanding of the data and their format and also show if (and how) we may need to wrangle them (further), it is difficult to make sense of the data just by looking at it.

---

## Making sense of data

To make sense of quantitative data we can reduce their information to unique values.

--

.center[
~ 

**That's a simple definition of summary statistics**

~]

--

As such, we can use summarizing functions of
- location (e.g., the mean),
- spread (e.g., standard deviation),
- the shape of the distribution (e.g., skewness), and
- relations between variables (e.g., correlation coefficients)

---

## Summary statistics: `summary()`

A quick and easy way to check some summary statistics for your dataset is the `base R` function `summary()` which can be applied to individual variables as well as whole dataframes:

```{r summary, eval = F}
summary(corona_survey$left_right)

summary(corona_survey[, 2:19])
```

.right[`r emo::ji("left_arrow_curving_right")`]

---

.smaller[
```{r ref.label = "summary", echo = F}

```
]

---

## Frequencies: `table()`

A simple way of looking at frequencies (e.g., for categorical variables) is the `base R` function `table()`.

```{r table}
table(corona_survey$choice_of_party)
```

If you also want to include `NA` in the frequency counts, you need to specify the argument `useNA = "always"`.

```{r table-NA}
table(corona_survey$choice_of_party, useNA = "always")
```

---

## Proportions with `prop.table()`

If you want proportions instead of raw counts, you can use the `base R` function `prop.table()`. You need to apply this function to an output produced by `table()`. 

.small[
```{r prop-table}
prop.table(table(corona_survey$choice_of_party))

prop.table(table(corona_survey$choice_of_party, useNA = "always"))
```
]

---

## Proportions with `prop.table()`

If you want fewer decimals places in the output, you can wrap the the `prop.table()` function in a `round()` call.

.small[
```{r round}
round(prop.table(table(corona_survey$choice_of_party, useNA = "always")), 3) # rounded to 3 decimal places

# if you want percentages
round((prop.table(table(corona_survey$choice_of_party, useNA = "always")) * 100), 2)
```
]

---

## Summary statistics: `psych::describe()`

For more detailed summary statistics for the numeric variables you can use the `describe()` function from the [`psych` package](https://cran.r-project.org/web/packages/psych/index.html).

```{r describe, eval = F}
library(psych)

corona_survey %>% 
  select_if(is.numeric) %>% 
  select(-id) %>% 
  describe() #<<
```

.right[`r emo::ji("left_arrow_curving_right")`]

---

.smaller[
```{r ref.label = "describe", echo = F, message = F}

```
]

---

## Summary statistics: `summarytools::descr()`

The [`summarytools` package](https://github.com/dcomtois/summarytools) provides a lot of functionalities for EDA, including the `descr()` function for summary statistics for numerical variables.

```{r descr, eval = F}
library(summarytools)

corona_survey %>% 
  select(left_right,
         starts_with("trust"),
         sum_measures,
         sum_sources,
         mean_trust) %>%
  descr(stats = "common") #<<
```

.right[`r emo::ji("left_arrow_curving_right")`]

---

class: center, middle

.smaller[
```{r ref.label = "descr", echo = F, message = F, warning = F}

```
]

---

## Summary statistics with `dplyr`

`dplyr` provides a helpful function for creating summary statistics: `summarize()`

`summarize()` is a vectorized function that can be used to create summary statistics for variables using functions like...

- `mean()`

- `sd()`

- `min()`

- `max()`

- etc.

A very nice thing about `summarize()` is that it produces a `tibble` which can be used for further analyses, plots, or to output tables (we will talk about the creation of tables in the session on `RMarkdown`).

---

## `dplyr::summarize()`

.small[
```{r summarize-example}
corona_survey %>% 
  summarize(
    mean_trust_gov = mean(trust_government, na.rm = TRUE),
    sd_trust_gov = sd(trust_government, na.rm = TRUE),
    var_trust_gov = var(trust_government, na.rm = TRUE),
    min_trust_gov = min(trust_government, na.rm = TRUE),
    max_trust_gov = max(trust_government, na.rm = TRUE)
  )
```
]

---

## `dplyr::group_by()`

The `dplyr` function `group_by()` creates dataframes (tibbles) that are grouped by one or more variables. This can, e.g., be used to produce grouped summary statistics. *Note:* To end/undo the grouping we can use the `ungroup()` function.

.small[
```{r group-by}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  group_by(choice_of_party) %>% #<<
   summarize(
    mean_trust_gov = mean(trust_government, na.rm = TRUE),
    sd_trust_gov = sd(trust_government, na.rm = TRUE),
    var_trust_gov = var(trust_government, na.rm = TRUE),
    min_trust_gov = min(trust_government, na.rm = TRUE),
    max_trust_gov = max(trust_government, na.rm = TRUE)
  )
  
```
]

---

## `dplyr::across()`

To produce grouped summary statistics for multiple variables you can use the `dplyr` function `across()`. *Note*: We only use cases without missing data for any of the variables here (= listwise deletion).

.smaller[
```{r across}
corona_survey %>%
  select(choice_of_party,
         starts_with("trust")) %>% 
  drop_na() %>% 
  group_by(choice_of_party) %>%
  summarize(across(starts_with("trust"), #<<
                   list(mean = mean, #<<
                        sd = sd), #<<
                   .names = "{col}_{fn}")) #<<
```
]

Note that we only use cases without missing data for any of the variables here.

---

## `dplyr::across()`

```{r, across-cartoon, out.width = "95%", echo = F}
include_graphics("./pics/dplyr_across.png")
```
<small><small>Illustration by [Allison Horst](https://github.com/allisonhorst/stats-illustrations) </small></small>

---

## Frequencies and proportions with `dplyr`

We can also use `group_by()` and `summarize()` to get frequencies and proportions for variables in our dataset.

```{r freqprop-dplyr}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  group_by(choice_of_party) %>% 
  summarize(n = n()) %>% 
  mutate(proportion = n/sum(n)) %>% 
  ungroup()
```

---

## Frequencies and proportions with `dplyr`

Instead of using `group_by` and `summarize()` to get frequency counts, we can also use `count()` from `dplyr` as a shorthand.

```{r freqprop-count}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(choice_of_party) %>% #<<
  mutate(proportion = n/sum(n)) %>% 
  ungroup()
```

---

## Frequencies and proportions with `janitor::tabyl()`

The [`janitor` package](https://github.com/sfirke/janitor) that we briefly mentioned in the section on *Data Wrangling* also provides a helpful function for creating frequency and proportion tables:

.small[
```{r tabyl, message = F}
library(janitor)

corona_survey %>% 
  tabyl(choice_of_party) %>% 
  adorn_pct_formatting(digits = 2, affix_sign = TRUE)
```
]

---

## Frequencies and proportions with `summarytools::freq()`

The `summarytools` package also includes the `freq()` function for frequency tables.

.small[
```{r summarytools-freq, message = F}
library(summarytools)

freq(corona_survey$choice_of_party)
```
]

---

## Relationships between variables

In addition to checking summary statistics for individual variables, another thing that you quite possibly also want to look at as part of your exploratory data analysis (EDA) are the relationships between (specific) variables in your dataset. There are many ways to do so and the appropriate choice of methods, of course, depends on the types of variables you want to explore. In the following, we will briefly discuss some options for two methods of exploring relationships between variables:

- crosstabulation

- correlations

---

## Crosstabs

Crosstabs can be used to explore relationships between categorical variables. As with almost everything in `R`, there are many different options for creating crosstabs, some of which we will discuss in the following. To start with, you can use the `base R` functions `table()` and `prop.table()` to generate crosstabs.

```{r base-crosstabs}
table(corona_survey$sex, corona_survey$choice_of_party) # rows, columns

round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party))*100, 2)
```

---

## Crosstabs

We can also calculate row or column percentages.

```{r base-crosstabs-margins}
round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party), 1)*100, 2) # row percentages
round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party), 2)*100, 2) # column percentages
```

If you want to generate tables based on more than two variables, the `base R` function `ftable()` is a good option for prettier printing of results.

---

## Crosstabs with `dplyr`

We can also use functions from `dplyr` in a similar fashion as we have done for a single variable to create crosstabs including frequencies and proportions.

```{r dplyr-crosstabs-freq}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(sex, choice_of_party) %>% 
  pivot_wider(names_from = choice_of_party,
              values_from = n)
```

---

## Crosstabs with `dplyr`

We can also use functions from `dplyr` in a similar fashion as we have done for a single variable to create crosstabs including frequencies and proportions.

```{r dplyr-crosstabs-prop}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(sex, choice_of_party) %>% 
  mutate(proportion = n/sum(n)*100) %>%
  select(-n) %>% 
  pivot_wider(names_from = choice_of_party,
              values_from = proportion)
```

---

## Other options for crosstabulation in `R`

Some of the interesting alternative options for crosstabs in `R` include the `CrossTable()` and `crosstab()` functions from the [`descr` package](https://cran.r-project.org/web/packages/descr/index.html), the latter of which also produces a mosaic plot to visualize the conditional frequencies, the `CrossTable()` function from the [`gmodels` package](https://cran.r-project.org/web/packages/gmodels/index.html), or the `ctable()` function from the [`summarytools` package](https://github.com/dcomtois/summarytools).

A very versatile option for crosstabs is the `tabyl()` function from the `janitor` package that we have introduced before.

---

## Crosstabs with the `janitor` package

The `tabyl()` function from the `janitor` package provides quite a few options for crosstabs. We will only show one example here, but you can learn more in the [`tabyl` vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).

.small[
```{r tabyl-crosstabs}
library(janitor)

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  tabyl(sex, choice_of_party) %>% 
  adorn_totals(where = c("row","col")) %>% 
  adorn_percentages(denominator = "row") %>% 
  adorn_pct_formatting(digits = 2) %>% 
  adorn_ns(position = "front")
```
]

---

## Chi-Square Test

You can, e.g., use the `summary()` function in combination with `table()` to do a chi-square test. The other packages mentioned previously that include functions for crosstabs can also be used for chi-square tests (e.g., the `janitor` package).

.small[
```{r chi-square}
# base R
summary(table(corona_survey$sex, corona_survey$choice_of_party))

# janitor
library(janitor)

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  tabyl(sex, choice_of_party) %>%
  chisq.test()
```
]

---

## Correlations

Again, as with the crosstabs examples, there are many different options for calculating and displaying correlations in `R`. In addition to the `base R` functions, we will look at two packages in this part: [`corrr`](https://corrr.tidymodels.org/) and [`correlation`](https://github.com/easystats/correlation).

---

# Correlations with `base R`

The `base R` function `cor()` computes the correlation coefficient(s) between two or more variables. This function can be used to calculate *Pearson's r*, *Kendall's tau*, and *Spearman's rho*. We also need to specify how we want to deal with missing values (e.g., use pairwise complete observations). For example, let's look at the correlations between the trust variables in our dataset:

.small[
```{r corr-base}
trust <- corona_survey %>% 
  select(starts_with("trust"))

cor(trust,
    use = "pairwise.complete.obs",
    method = "pearson")
```
]

---
## Correlations with `base R`

With `corr.test()` you can display the results of a significance test for a correlation.

```{r corr-test}
cor.test(trust$trust_rki, trust$trust_scientists, method = "pearson")
```

---

## The `corrr` package

The [`corrr` package](https://corrr.tidymodels.org/) is part of the [`tidymodels` suite of packages](https://www.tidymodels.org/) and provides various functions for displaying correlations. The main function is `correlate()` which produces a `tibble` as output.

.small[
```{r corr}
library(corrr)

correlate(trust)
```
]

---

## The `corrr` package

The `corrr` package provides several functions for tweaking/optimizing the output of the `correlate()` function. Here's one example:

.small[
```{r corr-output-tweaks}
trust %>% 
  correlate() %>% 
  rearrange() %>% 
  shave() %>% 
  fashion()
```
]

---

## The `corrr` package

.pull-left[
The package also provides different options for visualizing correlation coefficients.

```{r rplot, eval = F}
trust %>% 
  correlate() %>%
  rplot() #<<
```
]

.pull-right[
```{r ref.label = "rplot", echo = F, message = F}

```
]

---

## Plotting correlations with `GGally::ggcorr()`

The [`ggcorr` function](https://briatte.github.io/ggcorr/) from the [`GGally` package](https://ggobi.github.io/ggally/) - which is an extension to `ggplot2` -  also provides some interesting options for visualizing correlation coefficients.

```{r load-ggally}
library(GGally)
```

---

## `GGally::ggcorr()`

.small[
```{r ggcorr-example, out.width = "60%"}
trust %>% 
  ggcorr(label = TRUE,
         label_round = 2)
```
]

---

## The `correlation` package

The [`correlation` package](https://github.com/easystats/correlation) is part of the [`easystats` project](https://easystats.github.io/blog/portfolio/). It provides a much wider range of correlation types than the `base R` function `cor()` and the `correlate()` function from the `corrr` package. Its main workhorse is the `correlation()` function.

.small[
```{r correlation}
library(correlation)

correlation(trust)
```
]

---

## The `correlation` package

Among other things, the `correlation` package, e.g., also provides options for biserial and tetrachoric correlations for factors, and it also allows to calculate grouped/stratified correlations.

```{r correlation-grouped, eval = F}
corona_survey %>% 
  select(sex, starts_with("trust")) %>% 
  group_by(sex) %>% #<<
  correlation()
```

.right[`r emo::ji("left_arrow_curving_right")`]

---

class: center, middle

.smaller[
```{r ref.label = "correlation-grouped", echo = F}

```
]

---

class: center, middle

# [Exercise](https://jobreu.github.io/r-intro-gesis-2020/exercises/Day4_1_EDA_Exercise_1_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/r-intro-gesis-2020/solutions/Day4_1_EDA_Exercise_1_solution.html)

---

## Guilty by ~~association~~ correlation

.column-left-half[
While correlation coefficients are useful for exploring relationships between variables, they can also be misleading. For example, if we do correlation analysis and we encounter a (Pearson's) correlation coefficient close to 0, we often think of relationships as pictured on the right side.
]

.column-right-half[
```{r dino-plot-1, out.width = "80%", echo = F}
library(datasauRus)

datasaurus_dozen %>% 
  filter(dataset == "h_lines") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```
]

---

## Guilty by ~~association~~ correlation

This dataset has **the same correlation coefficient (Pearson's r of -0.06)** as the one on the previous slide:

```{r dino-plot-2, out.width = "50%", echo = F}
datasaurus_dozen %>% 
  filter(dataset == "slant_up") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```

---

## Guilty by ~~association~~ correlation `r emo::ji("T-Rex")`

So does this one...

```{r dino-plot-3, out.width = "60%", echo = F}
datasaurus_dozen %>% 
  filter(dataset == "dino") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```

---

## Guilty by ~~association~~ correlation

We could go on... The previous three examples all come from the [`datasauRus` package](https://github.com/lockedata/datasauRus) which essentially is an extension of [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) and includes 13 datasets with the same (Pearson) correlation between x and y.

```{r dino-plot-4, out.width = "55%", echo = F}
datasaurus_dozen %>% 
ggplot(aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap(~dataset, ncol=3)
```

---

## Dinosaur `r emo::ji("T-Rex")` vs. Shark `r emo::ji("shark")`

```{r load-shark, echo = F, message= F}
shark <- read_csv("../../data/shark_raw.csv")
```

.pull-left[
```{r t-rex, echo = F, out.width="75%"}
datasaurus_dozen %>% 
  filter(dataset == "dino") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```

.small[
```{r t-rex-corr}
datasaurus_dozen %>% 
  filter(dataset == "dino") %$% 
  cor(x, y)
```
]
]

.pull-right[
```{r shark, echo = F, out.width="75%"}
shark %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```
<small><small>Data shark by [Allison Horst](https://github.com/allisonhorst/stats-illustrations) </small></small>

.small[
```{r shark-corr}
shark %$% 
  cor(x, y)
```
]
]

--

.center[
***Sharks are more ~~coral-hated~~ correlated!***
]

.smaller[PS: You can make your own data art with [drawdata](https://drawdata.xyz/).]

---

## Trust no singular value!

Importantly, the x- and y-variables in these `datasaurus_dozen` dataset also all have the same means and standard deviations.

.smaller[
```{r datasaurus-summary-stats}
datasaurus_dozen %>% 
  group_by(dataset) %>%
  summarize(
    mean_x = mean(x), 
    mean_y = mean(y), 
    sd_x = sd(x), 
    sd_y = sd(y), 
    corr = cor(x, y, method = "pearson")
  )
```
]

---

## Plot your data!

The message from the `datasaurus_dozen` examples should be clear. Relying only on singular values that summarize the location or spread of a single variable or the association of two variables is not a good idea. To avoid reducing a ~~mountain to a molehill~~ dinosaur to a lack of correlation, it is important to plot your data as part of your EDA to explore:

- univariate distributions

- grouped univariate distributions (if you have and want to compare groups)

- bivariate distributions

There are many good resources for plotting with `R`. A very exhaustive and approachable one is [*From Data to Viz*](https://www.data-to-viz.com/) which provides a collection of recipes for a lot of different plot types (for both `R` and `Python`). Many of the following examples are based on those. 

---

## EDA vs. publication-ready plots

When you get more familiar with plotting in `R` you will notice that there are seemingly unlimited plotting options that allow for comprehensive customization. This means that you can spend a lot of time on tweaking and optimizing plot. While this makes sense for producing figures for publications, in the context of EDA, the figures you produce are meant for yourself (plus maybe a few collaborators). Hence, we will mostly make use of (rather) basic examples in the following. Although `base R` provides quite a few plotting options, we will focus using `ggplot2` (and extensions thereof) as it is generally more versatile.

---

## Plotting univariate distributions

In the following, we will discuss some examples of plotting univariate distributions of

1) categorical variables

2) numeric variables

These plots can be created for a whole dataset or to compare specific subsets (groups) within the data.

Of course, there are more options than the ones we will show for plotting univariate distributions. Which one(s) to choose depends on the specific kind of data you have, the question you want to answer with the data, and also - to some degree - personal preferences or what is customary in your field.

---

## Bar chart

.pull-left[
The most common way of displaying the distribution of one categorical variable is a bar chart. The simplest way of creating a bar chart that shows counts per category with `ggplot2` is:

.small[
```{r bar-chart-basic, eval = F}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party)) +
  geom_bar() #<<
```
]
]

.pull-right[
```{r ref.label = "bar-chart-basic", echo = F}
```
]

---

## Bar chart: Add colors

.pull-left[
We can also have different colors for the categories in our bar chart:

.small[
```{r bar-chart-color, eval = F}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             fill = choice_of_party)) + #<<
  geom_bar()
```
]

*Note*: There are many ways of choosing and customizing the colors in a bar chart. However, as our focus is on plots for EDA, we will not cover these options here.
]

.pull-right[
```{r ref.label = "bar-chart-color", echo = F}
```
]

---

## Row chart

.pull-left[
While we are quite used to seeing bar charts, flipping their coordinates, so that they become row charts, can make it easier to read them (at least for people from countries in which the reading direction is left to right, top to bottom).

.small[
```{r bar-chart-flip, eval = F}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             fill = choice_of_party)) + 
  geom_bar() +
  coord_flip() #<<
```
]
]

.pull-right[
```{r ref.label = "bar-chart-flip", echo = F}
```
]

---

## "We're last, meaning we're first..."

```{r trump, out.width = "85%", echo = FALSE}
knitr::include_graphics("./pics/trump_charts.jpg")
```

.footnote[https://twitter.com/JoJoFromJerz/status/1290630319713001472]

---

## Change the order of categories

.pull-left[
There are [several ways of changing the order of categorical variable values for a plot](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html). In the previous example, we might want to change the order of parties from top to bottom based on their counts. To do this we can, e.g., use the `fct_rev()` and `fct_infreq()` functions from the `forcats` package.

.small[
```{r bar-chart-order, eval = F}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = fct_rev(fct_infreq(choice_of_party)), #<<
             fill = choice_of_party)) + 
  geom_bar() +
  xlab("Party preference") +
  ylab("Count") +
  coord_flip()
```
]
]

.pull-right[
```{r ref.label = "bar-chart-order", echo = F}
```
]

---

## Bar or row chart with percentages

.pull-left[
We might want to display relative frequencies (proportions) instead of absolute frequencies (counts) for our categorical variable.

.small[
```{r bar-chart-perc, eval = F}
library(scales)

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             fill = choice_of_party)) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) + #<<
  scale_y_continuous(labels=scales::percent) + #<<
  ylab("Relative Frequencies")
```
]
]

.pull-right[
```{r ref.label = "bar-chart-perc", echo = F, message = F}
```
]

---

## Pretty bar plot

.pull-left[
Although we don't want to (and can't) discuss all plot customization option in detail, we want to show one example of a heavily tweaked bar chart to illustrate what is possible.

.smaller[
```{r bar-chart-custom, eval = F}
colors <- c("#000000", "#E30013", "#FFEE00", "#BE3075", "#19A329", "#009FE1", "#808080")

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             fill = choice_of_party)) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  geom_text(aes(y = (..count..)/sum(..count..), 
              label = paste0(round(prop.table(..count..) * 100, 2), '%')),
            stat = 'count',
            size = 3,
            vjust = -.5) +
  scale_y_continuous(labels = scales::percent,
                     expand = expansion(mult=c(0,0.1))) +
  scale_fill_manual(values = colors) +
  labs(title = "Which party would you vote for if federal elections were held next Sunday?",
       x = "",
       y = "",
       caption = "Note: N = 2783 \nSource: GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany.\nGESIS Datenarchiv, KÃ¶ln. ZA5667 Datenfile Version 1.1.0, https://doi.org/10.4232/1.13520.") +
  theme(legend.position = "none",
        panel.grid.major.x = element_blank(),
        axis.ticks.x = element_blank())
```
]
]

.pull-right[
```{r ref.label = "bar-chart-custom", echo = F, warning = F}
```
]

---

## Alternatives to bar charts

There are [several alternatives to bar charts](https://towardsdatascience.com/anything-but-bars-the-10-best-alternatives-to-bar-graphs-fecb2aaee53a) for plotting categorical variables. An interesting one is the so-called [donut chart](https://www.r-graph-gallery.com/128-ring-or-donut-plot.html) which can, e.g., be created with the `ggdonutchart()` function from the [`ggpubr` package](https://rpkgs.datanovia.com/ggpubr/index.html).

---

## Discrete numeric variables

.pull-left[
For plotting the univariate distribution of discrete numeric variables (such as those based on Likert scales) it is, of course, possible to also use bar charts.

.small[
```{r bar-chart-num, eval = F}
corona_survey %>% 
  filter(!is.na(trust_scientists)) %>% 
  ggplot(aes(x = trust_scientists)) +
  geom_bar()
```
]
]

.pull-right[
```{r ref.label = "bar-chart-num", echo = F, message = F}
```
]

---

## Discrete numeric variables

.pull-left[
Bar plots can become a bit unwieldy for visualizing the distribution of discrete numeric variables, if the number of individual values is high. In cases where there is a very high number of different values in a discrete numeric variable it may be better to use histograms instead of bar plots.

.small[
```{r bar-chart-num2, eval = F}
corona_survey %>% 
  filter(!is.na(left_right)) %>% 
  ggplot(aes(x = left_right)) +
  geom_bar()
```
]
]

.pull-right[
```{r ref.label = "bar-chart-num2", echo = F, message = F}
```
]

---

## Special case: Likert-plots

For plotting the shares of responses to Likert-type items, the `plot_likert()` function from the [`sjPlot` package](https://strengejacke.github.io/sjPlot/) is an interesting option. However, this requires labelled data. Another option is the [`likert` package](https://github.com/jbryer/likert).

---

## Continuous numeric variables

The univariate distribution of a continuous numeric variable can be plotted as a histogram. As there are no continuous variables in the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* dataset, we will use data from [*Gapminder*](https://www.gapminder.org/) on worldwide life expectancy and GDP per capita in 2018. As always, we first need to load and wrangle the data.

.smaller[
```{r gapminder-data-display, eval = F}
life_exp_2018 <- read_csv("./data/life_expectancy_years.csv") %>%
  pivot_longer(-country,
               names_to = "year",
               values_to = "life_exp") %>% 
  filter(year == "2018") %>% 
  select(-year)

gdp_pc_2018 <- read_csv("./data/gdppercapita_us_inflation_adjusted.csv") %>%
  pivot_longer(-country,
               names_to = "year",
               values_to = "gdp_percap") %>% 
  filter(year == "2018") %>% 
  select(-year)

gapminder_2018 <- life_exp_2018 %>% 
  left_join(gdp_pc_2018, by = "country")
```

```{r gapminder-data, echo = F, message = F}
life_exp_2018 <- read_csv("../../data/life_expectancy_years.csv") %>%
  pivot_longer(-country,
               names_to = "year",
               values_to = "life_exp") %>% 
  filter(year == "2018") %>% 
  select(-year)

gdp_pc_2018 <- read_csv("../../data/gdppercapita_us_inflation_adjusted.csv") %>%
  pivot_longer(-country,
               names_to = "year",
               values_to = "gdp_percap") %>% 
  filter(year == "2018") %>% 
  select(-year)

gapminder_2018 <- life_exp_2018 %>% 
  left_join(gdp_pc_2018, by = "country")
```
]

---

## Histogram

.pull-left[
Now we can use the *Gapminder* data on global life expectancy in 2018 to create a histogram.

.small[
```{r hist, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp)) %>% 
  ggplot(aes(x = life_exp)) +
  geom_histogram() #<<
```
]
]

.pull-right[
```{r ref.label = "hist", echo = F, message = F}
```
]

---

## Histogram

.pull-left[
One thing to keep in mind about histograms is that they can look quite different for the same data, depending on the number and size of the bins they use. We can easily adapt this with `ggplot2` by specifying the `binwidth` or the `bins` argument.

```{r hist-binwidth, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp)) %>% 
  ggplot(aes(x = life_exp)) +
  geom_histogram(binwidth = 5) #<<
```
]

.pull-right[
```{r ref.label = "hist-binwidth", echo = F, message = F}
```
]

---

## Density plot

.pull-left[
To avoid the issue of picking the "right" number or size of bins, a density plot can be a good alternative to a histogram for continuous numeric variables.

*Note*: If you want combine multiple density plots (e.g., for different groups), you can create ridgeline plots, e.g., with the [`ggridges` package](https://github.com/wilkelab/ggridges).

.small[
```{r density, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp)) %>% 
  ggplot(aes(x = life_exp)) +
  geom_density(fill="#69b3a2", #<<
               color="#e9ecef") #<<
```
]
]

.pull-right[
```{r ref.label = "density", echo = F, message = F}
```
]

---

## Testing the normality assumption

Some statistical tests require that the data (roughly) follow a normal distribution. Histograms and density plots can already help us get a good first impression if our data are (approximately) normally distributed. Two other options for visually testing the normal distribution of our data are [QQ-plots and PP-plots](https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/qqpp.html#pp-plots). While [`ggplot` offers geoms for QQ-plots](https://ggplot2.tidyverse.org/reference/geom_qq.html), the [`qqplotr` package](https://github.com/aloy/qqplotr) offers a lot of convenient functions and options for creating and customizing QQ- and PP-plots.

---

## QQ-Plot

.pull-left[
The QQ-plot plots data quantiles against theoretical quantiles (e.g., of the normal distribution). If our data are (approximately) normally distributed, they should follow the diagonal line in the QQ-plot, or at least be within its confidence band. Let's create a QQ-plot for the political orientation variable from the *GESIS Panel* dataset.

.small[
```{r qqplot-gp, eval = F}
library(qqplotr)

corona_survey %>% 
  filter(!is.na(left_right)) %>% 
  ggplot(mapping = aes(sample = left_right)) +
  stat_qq_band() +
  stat_qq_line() +
  stat_qq_point() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles") 
```
]
]

.pull-right[
```{r ref.label = "qqplot-gp", echo = F, message = F}
```
]

---

## QQ-Plot

.pull-left[
The reason the QQ-plot for the political orientation variable looks the way it does is that the values of the variable are discrete. The ~~picture~~ plot looks different, if we use a continuous variable.

.small[
```{r qqplot-gapminder, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp)) %>% 
  ggplot(mapping = aes(sample = life_exp)) +
  stat_qq_band() +
  stat_qq_line() +
  stat_qq_point() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles") 
```
]
]

.pull-right[
```{r ref.label = "qqplot-gapminder", echo = F, message = F}
```
]

---

## PP-Plot

.pull-left[
The PP-plot compares the proportion of values between the actual and a theoretical (in this case normal) distribution.

.small[
```{r ppplot-gapminder, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp)) %>% 
  ggplot(mapping = aes(sample = life_exp)) +
  stat_pp_band() +
  stat_pp_line() +
  stat_pp_point() +
  labs(x = "Probability Points",
       y = "Cumulative Probability") 
```
]
]

.pull-right[
```{r ref.label = "ppplot-gapminder", echo = F, message = F}
```
]

---

## Normality test

`Base R` also provides functions for statistical tests of normality. The most common ones are *Shapiro-Wilk's test* and the *Kolmogorov-Smirnov* test, both of which test the H0 that the data follow a normal distribution against the H1 that they do not follow a normal distribution. It should be noted, however, that, especially for larger sample sizes, these tests tend to be quite conservative.

.small[
```{r normality-tests, warning = F}
shapiro.test(gapminder_2018$life_exp)

ks.test(gapminder_2018$life_exp, 
        "pnorm",
        mean = mean(gapminder_2018$life_exp, na.rm = TRUE),
        sd = sd(gapminder_2018$life_exp, na.rm = TRUE))
```
]

---

## Visual group comparisons

If we want to visually compare distributions (or summary statistics) across groups or, more generally speaking, across different values of categorical variables in our dataset, there are plenty of options in `R`. There are grouped versions of all of the plots we discussed before: bar plots, histograms, and density plots. However, there also are other plot types that are especially suited for comparing subsets/groups in our data. 

*Note*: The [`ggpubr` package](https://rpkgs.datanovia.com/ggpubr/index.html) and the [`ggstatsplot` package](https://github.com/IndrajeetPatil/ggstatsplot) include some good options for visualizations of group comparisons. 

---

## Grouped & stacked bar plots

.pull-left[
If we want to compare the distribution of categorical variables across groups, we can use grouped or stacked bar plots. If, e.g., we want to compare the party preferences across education categories in the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* dataset, we could do so with a grouped bar plot.

.small[
```{r grouped-barplot, eval = F}
corona_survey %>%
  filter(!is.na(choice_of_party)) %>%
  ggplot(aes(x = education_cat, 
             fill = choice_of_party)) +
  geom_bar(position="dodge") #<<
```
]
]

.pull-right[
```{r ref.label = "grouped-barplot", echo = F}
```
]

---

## Grouped & stacked bar plots

.pull-left[
If, e.g., we want to compare the share of female and male voters for each party, we can use a stacked bar plot.

.small[
```{r stacked-barplot, eval = F}
corona_survey %>%
  filter(!is.na(choice_of_party)) %>%
  ggplot(aes(x = choice_of_party, 
             fill = sex)) +
  geom_bar(position="fill") #<<
```
]
]

.pull-right[
```{r ref.label = "stacked-barplot", echo = F}
```
]

---

## Bar plot with error bars

.pull-left[
One way to compare, e.g., mean values across groups is a bar plot with error bars, sometimes also called a dynamite plot.

.small[
```{r barplot-error, eval = F}
corona_survey %>% 
  filter(!is.na(trust_government),
         !is.na(choice_of_party)) %>% 
  group_by(choice_of_party) %>% 
  summarize(trust_in_government = mean(trust_government),
            sd = sd(trust_government)) %>% 
  ggplot() +
    geom_bar(aes(x = choice_of_party,
                 y = trust_in_government),
             stat="identity", 
             fill="skyblue") +
    geom_errorbar(aes(x = choice_of_party,
                      ymin = trust_in_government - sd,
                      ymax = trust_in_government + sd), 
                  width=0.4,
                  colour="orange",
                  size=1)
```
]
]

.pull-right[
```{r ref.label = "barplot-error", echo = F, message = F}
```
]

---

## Boxplot

.pull-left[
A plot type that, despite having [received quite some criticism in recent years](https://www.data-to-viz.com/caveat/boxplot.html), is still widely used to visually summarize numeric data is the boxplot. `ggplot2` provides its own geom for this type of plot.

.small[
```{r boxplot-basic, eval = F}
corona_survey %>%
  filter(!is.na(trust_government),
         !is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             y = trust_government)) +
    geom_boxplot() #<<
```
]
]

.pull-right[
```{r ref.label = "boxplot-basic", echo = F, message = F}
```
]

---

## Show me your data points!

```{r, summary-cartoon, out.width = "95%", echo = F}
include_graphics("./pics/summary_statistics.png")
```
<small><small>Illustration by [Allison Horst](https://github.com/allisonhorst/stats-illustrations) </small></small>

---

## Boxplot with individual data points

.pull-left[
One key point of criticism regarding the use of boxplots as well as dynamite plots (bar plots with error bars) is that they do not provide any information about the distribution of the data. This has even led some scientists to start an initiative to [#barbarplots](https://barbarplots.github.io/index.html). However, with `ggplot2` information about the distribution of variables can easily be added to a boxplot (or other types of plots), e.g., with the `geom_jitter`.

.small[
```{r boxplot-jitter, eval = F}
corona_survey %>%
  filter(!is.na(trust_government),
         !is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             y = trust_government)) +
    geom_boxplot() +
    geom_jitter() #<<
```
]
]

.pull-right[
```{r ref.label = "boxplot-jitter", echo = F, message = F}
```
]

---

## Violin plots

.pull-left[
The previous version of adding individual data points to the boxplot was not really visually appealing. A good alternative to boxplots are violin plots which show the distribution of the variable (within groups) and can be combined with boxplots.

.small[
```{r violin-box, eval = F}
corona_survey %>%
  filter(!is.na(trust_government),
         !is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             y = trust_government)) +
    geom_violin(aes(fill = choice_of_party,
                    color = choice_of_party),
                width=1) +
    geom_boxplot(width=0.1) +
    theme(legend.position="none") +
    xlab("")
```
]
]

.pull-right[
```{r ref.label = "violin-box", echo = F, message = F}
```
]

---

## Half-violin plots

.pull-left[
Violin plots include a mirrored density plot. An alterntive to this are half-violin plot that can, e.g., be created with the [`gghalves` package](https://github.com/erocoar/gghalves).

*Note*: An extension of these half-violin plots are [raincloud plots](https://github.com/RainCloudPlots/RainCloudPlots) that combine density, dot, and boxplots.

.small[
```{r half-violin, eval = F}
library(gghalves)

corona_survey %>%
  filter(!is.na(trust_government),
         !is.na(choice_of_party)) %>% 
  ggplot(aes(x = choice_of_party, 
             y = trust_government)) +
    geom_half_violin(aes(fill = choice_of_party,
                         color = choice_of_party)) +
    geom_half_point(aes(color = choice_of_party),
                        transformation = position_jitter()) +
    theme(legend.position="none") +
    xlab("")
```
]
]

.pull-right[
```{r ref.label = "half-violin", echo = F, message = F}
```
]

---

## Plotting bivariate distributions of numeric variables

Similar to univariate statistics and distributions, there are multiple options for plotting bivariate distributions in `R`. Again, [*From Data to Viz*](https://www.data-to-viz.com/) is a good resource to consult here. We will explore some of the options for plotting bivariate distributions of numeric variables in the following.

---

## Scatter plot

.pull-left[
A very common option for plotting bivariate distributions to explore relationships between numeric variables is the scatter plot.

.small[
```{r scatter-plot, eval = F}
gapminder_2018 %>% 
  filter(!is.na(life_exp),
         !is.na(gdp_percap)) %>% 
  ggplot(aes(x = gdp_percap,
             y = life_exp)) +
  geom_point()
```
]
]

.pull-right[
```{r ref.label = "scatter-plot", echo = F, message = F}
```
]

---

## Scatter plot + marginal distributions

.pull-left[
The `ggMarginal()` function from the [`ggExtra` package](https://github.com/daattali/ggExtra) allows us to add information about the distribution of variables to the margins of a scatter plot. We can add histograms, density plots or boxplots.

.small[
```{r scatter-margin, eval = F}
library(ggExtra)

p <- gapminder_2018 %>% 
  filter(!is.na(life_exp),
         !is.na(gdp_percap)) %>% 
  ggplot(aes(x = gdp_percap,
             y = life_exp)) +
  geom_point()

  ggMarginal(p, type = "histogram")
```
]
]

.pull-right[
```{r ref.label = "scatter-margin", echo = F, message = F}
```
]
---

## Extensions

Of course, it is also to include more than just two variables in a plot. For example, you can create grouped scatter plots and map the size of the plots in a scatter plot to another variable (e.g., the population of a country in the *Gapminder* examples). If you want to (visually) explore time series data, [*From Data to Viz*](https://www.data-to-viz.com/) has a section on this or you could, e.g. have a look at the [`timetk` package](https://business-science.github.io/timetk/index.html).

There are also several extension packages for `ggplot2` that provide additional plot types or (convenient) plotting options. For example, the following packages provide some interesting (additional) options for EDA: [`ggExtra`](https://github.com/daattali/ggExtra), [`GGAlly`](https://ggobi.github.io/ggally/), [`ggpubr`](https://rpkgs.datanovia.com/ggpubr/index.html) 

---

## Missings & outliers

Two things that can influence summary statistics as well as univariate and bivariate distributions are missings and outliers. Hence, before we can analyze our data, we should check whether we have clear patterns of missingness or extreme outliers in our data.

---

## Missing data

In the *Data Wrangling* session we have already discussed how we can define specific values as missings (`NA`) and how we can recode `NA` into something else. As we have seen in that session, the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* dataset contains quite a few codes for different types of missing data. However, when we collect data ourselves or if we (re-)use datasets that are not as well-documented as the *GESIS Panel* data, we may need to explore potential patterns of missingness to see, if there may have been identifiable reasons why data for certain variables and/or observations are missing. There is a [vignette for the `naniar` package](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html) that provides a good overview of different missing data visualizations. We will explore some of those in the following.

---

## `visdat::vis_dat()`

.pull-left[
The [`visdat` package](http://visdat.njtierney.com/) provides several functions for visualizing missind data.

```{r visdat, eval = F}
library(visdat)

vis_dat(corona_survey)
```
]

.pull-right[
```{r ref.label = "visdat", echo = F, message = F}
```
]

---

## `visdat::vis_dat()`

.pull-left[
The `vis_miss()` function from the `visdat` package provides some further details on missing values in a dataset.

```{r vis-miss, eval = F}
vis_miss(corona_survey)
```
]

.pull-right[
```{r ref.label = "vis-miss", echo = F, message = F}
```
]

---

## `UpSetR::gg_miss_upset()`

.pull-left[
The [`UpSetR` package](https://github.com/hms-dbmi/UpSetR) includes the `gg_miss_upset()` function which allows some in-depth exploration of missingness patters in a dataset.

```{r gg-miss-upset, eval = F}
library(UpSetR)

gg_miss_upset(trust)
```
]

.pull-right[
```{r ref.label = "gg-miss-upset", echo = F, message = F}
```
]

---

## `naniar::gg_miss_var()`

.pull-left[
The [`naniar` package](http://naniar.njtierney.com/index.html) that we have used to replace specific values with `NA` in our data also provides a set of functions for visualizing missing data. The first one we want to look at is `gg_miss_var()` which shows the number or percentage of missing values for each variable in a dataset.

```{r gg-miss-var, eval = F}
library(naniar)

gg_miss_var(corona_survey,
            show_pct = TRUE)
```
]

.pull-right[
```{r ref.label = "gg-miss-var", echo = F, message = F}
```
]

---

## `naniar::gg_miss_case()`

.pull-left[
The complement to `gg_miss_var()` is `gg_miss_case()` which shows the number or percentage of missing values for each case/observation in a dataset.

```{r gg-miss-case, eval = F}
library(naniar)

gg_miss_case(corona_survey,
            show_pct = TRUE)
```
]

.pull-right[
```{r ref.label = "gg-miss-case", echo = F, message = F}
```
]

---

## `naniar::gg_miss_fct()`

.pull-left[
The `gg_miss_fct()` function creates a plot that shows the percentage of missing values for all (other) variables for different values of a categorical.

.small[
```{r gg-miss-fct, eval = F}
library(naniar)

gg_miss_fct(corona_survey,
            fct = choice_of_party)
```
]
]

.pull-right[
```{r ref.label = "gg-miss-fct", echo = F, message = F}
```
]

---

## Outliers

There are many ways of identifying and dealing with outliers. In the following, we will look at three of them: box/violin plots, interquartile range (IQR), and Mahalanobis distance.

*Note*: An alternative method for detecting outliers based on the Minimum Covariance Determinant is described in a [blog post by Will Hipson](https://willhipson.netlify.app/post/outliers/outliers/).

---

## Detecting outliers: Box/violin plots

.pull-left[
The [`ggstatsplot` package](https://github.com/IndrajeetPatil/ggstatsplot) includes the `ggbetweenstats()` function for creating a combined violin and boxplot that tags outliers.

*Note*: Installing the `ggstatsplot` package will also install a lot of additional packages (which may take some time).

.small[
```{r ggbetweenstats, eval = F}
library(ggstatsplot)

ggbetweenstats(data = corona_survey, 
               x = sex,
               y = left_right,
               outlier.tagging = TRUE,
               outlier.label = id,
               results.subtitle = FALSE)
```
]
]

.pull-right[
```{r ref.label = "ggbetweenstats", echo = F, message = F}
```
]

---

## Detecting outliers: IQR

For identifying univariate outliers we can specify lower and upper cutoffs, e.g., using the formula 25th percentile - 1.5 * interquartile range (IQR) for the lower and 75th percentile + 1.5 * IQR for the upper limit. In `R` this can be done as follows:

.small[
```{r iqr}
q2575 <- quantile(gapminder_2018$gdp_percap, 
                  probs=c(.25, .75), 
                  na.rm = TRUE)

iqr <- IQR(gapminder_2018$gdp_percap,
           na.rm = TRUE)

ul <-  q2575[2]+1.5*iqr  
ll <- q2575[1]-1.5*iqr

gapminder_2018_cut <- gapminder_2018 %>%
  filter(gdp_percap <= ul)

```
]

**NB**: The value of 1.5 x IQR is a rule of thumb. You should always check whether this makes sense for your data. In the above example with the *Gapminder* data, e.g., it most likely does not. An alternative approach is using mean + 2 x SD (or 3 x SD), but, again, this is nothing more than a rule of thumb.

---

## Detecting outliers: Mahalanobis distance

.pull-left[
A common method for identifying multivariate outliers is Mahalanobis distance. The `outlier()` function from the `psych` package calculates and plots Mahalanobis distance.

*Note*: You can also [calculate Mahalanobis distance to identify and filter outliers](https://willhipson.netlify.app/post/outliers/outliers/) using the `base R` function `mahalanobis()`.

.small[
```{r md, eval = F}
library(psych)

out <- outlier(gapminder_2018[, -1])
```
]
]

.pull-right[
```{r ref.label = "md", echo = F, message = F}
```
]

---

## EDA packages

There are quite a few packages that are made for easy EDA or include functions that facilitate EDA. For example: [`inspectdf`](https://alastairrushworth.github.io/inspectdf/), [`skimr`](https://docs.ropensci.org/skimr/), [`summarytools`](https://github.com/dcomtois/summarytools), [`dataMaid`](https://github.com/ekstroem/dataMaid), [`DataExplorer`](https://boxuancui.github.io/DataExplorer/), [`descriptr`](https://descriptr.rsquaredacademy.com/index.html), [`GGally`](https://ggobi.github.io/ggally/index.html)

Going through all of them would be too much. However, we will highlight a few exemplary functions from these packages for automated/simplified EDA that combine several of the different steps and options we covered in this session in the following.

---

## `skimr::skim()`

```{r skim, eval = F}
library(skimr)

skim(corona_survey[, -1])
```

.right[`r emo::ji("left_arrow_curving_right")`]

---

.smaller[
```{r ref.label = "skim", echo = F, message = F}

```
]

---

## `GGally::ggpairs()`

.pull-left[
.small[
```{r ggpairs, eval = F}
library(GGally)

corona_survey %>% 
  select(sex,
         education_cat,
         risk_self,
         risk_surround,
         trust_government,
         trust_rki,
         trust_scientists,
         sum_measures) %>% 
  ggpairs()
```
]
]

.pull-right[
```{r ref.label = "ggpairs", echo = F, message = F, warning = F}
```
]

---

## `inspectdf::inspect_cat()`

.pull-left[
.small[
```{r inspect-cat, eval = F}
library(inspectdf)

cat <- inspect_cat(corona_survey)

show_plot(cat)
```
]
]

.pull-right[
```{r ref.label = "inspect-cat", echo = F, message = F}
```
]

---

class: center, middle

# [Exercise](https://jobreu.github.io/r-intro-gesis-2020/exercises/Day4_1_EDA_Exercise_2_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/r-intro-gesis-2020/solutions/Day4_1_EDA_Exercise_2_solution.html)

---

# Extracurricular activities

Further explore the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* dataset and develop some ideas for interesting research questions that you could answer by analyzing it with `R`.