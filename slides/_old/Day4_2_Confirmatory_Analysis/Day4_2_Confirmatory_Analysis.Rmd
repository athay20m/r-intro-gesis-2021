---
title: "Introduction to R for Data Analysis"
subtitle: "Confirmatory Analysis"
author: "Johannes Breuer<br />Stefan JÃ¼nger"
date: "2020-08-06"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
  lib_dir: libs
css: ["default", "default-fonts", "../workshop.css"]
nature:
  highlightStyle: "github"
highlightLines: true
countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---
layout: true

```{r setup, include = FALSE}
source("../xaringan_r_setup.R") 
options(warn=-1)
xaringanExtra::use_xaringan_extra(c("tile_view", "clipboard"))
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
# gp_covid <-
#   haven::read_sav(
#     "./data/gesis_panel_covid19/ZA5667_v1-1-0.sav"
#   ) %>%
#   sjlabelled::set_na(na = c(-1:-99, 97)) %>%
#   dplyr::mutate(
#     likelihood_infection = hzcy001a,
#     age_cat = as.factor(age_cat)
#   )
```

<div class="my-footer">
<div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
<div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
<div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>
  
```{css, echo = FALSE}
.tinyish .remark-code { /*Change made here*/
    font-size: 70% !important;
}

.tinyisher .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

---

## Content of this sessions
.pull-left[
**What we will cover**
- Bivariate hypothesis testing

- Using more or less simple regression models in `R`
  - OLS, GLM, and the like
  
- How to re-use the results of these models

- How to feed these results into tables and pretty plots
]

.pull-right[
**What we won't cover**
- Theory (and history) of hypothesis testing

- Crazy complex models with elaborated estimators
  - e.g., no multilevel models
  - also no clustered standard errors
  
- Bayesian statistics
]

---

## Data in this session
In this session, we, again, use the data from *GESIS Panel* COVID-19 survey:

```{r load-gp-data, echo = FALSE}
gp_covid <-
  haven::read_sav(
    "../../data/ZA5667_v1-1-0.sav"
  ) %>% 
  sjlabelled::set_na(na = c(-1:-99, 97)) %>% 
  dplyr::mutate(
    likelihood_infection = hzcy001a,
    age_cat = as.factor(age_cat)
  ) %>% 
  sjlabelled::remove_all_labels()
```

```{r load-gp-data-display, eval = FALSE}
gp_covid <-
  haven::read_sav(
    "./data/ZA5667_v1-1-0.sav"
  ) %>% 
  sjlabelled::set_na(na = c(-1:-99, 97)) %>% 
  dplyr::mutate(
    likelihood_infection = hzcy001a,
    age_cat = as.factor(age_cat)
  ) %>% 
  sjlabelled::remove_all_labels()
```

---

## `R` is rich in statistical procedures
Generally, if you seek to use a specific statistical method in `R`, chances are quite high that you can easily do that. 
- As we said before: There's ~~an app~~a package for that
  - Either directly on *CRAN* or, for example, on *GitHub*
  
For my use cases, I have only vers rarely encountered situations in which I cannot use a specific  procedure in `R`. Here's my small collection:
- Structural Equation Models (SEM) using categorical latent variables
- Hypothesis testing of marginal effects across different models
- ...

In principle, you could, of course, program these things yourself...

---

## Plenty of options
A lot can be done in `R` with regard to statistical analysis as it is the number one language for statisticians who develop cutting-edge methods.

Before we start analyzing data, however, we have to make ourselves familiar with some more terminology in `R`.

---

## Formulas in statistical software
We have seen this before. As in other statistical languages, e.g., regression models require you to define your dependent variable and your independent ones. For example, in *Stata* you have to write:

```{r eval = FALSE}
y x1 x2 x3
```

*SPSS* is more literate by requiring you to state what your dependent variables are with the `/DEPENDENT` parameter.

---

## `R` is straightforward and literate
`R` combines the best of two worlds: It is straightforward to write formulas and it is quite literate regarding what role a specific element of a formula plays.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

*Note*: Formulas represent a specific object class in `R`.

```{r}
class(y ~ x1 + x2 + x3)
```

---

## Denoting the left-hand side with `~`
In `R`, stating what your dependent variable is is really similar to some fancy flavors of mathematical notation:

$$y \sim N(\theta, \epsilon)$$

It states that a specific relationship is actually _estimated_, but we, fortunately, don't have to specify errors here.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

Yet, sometimes it may be a good idea at least to specify the intercept as here:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3
```

---

## ...since being explicit regarding the intercept is bliss
We can estimate models without an intercept:

```{r eval = FALSE}
y ~ x1 + x2 + x3 - 1
```

Or intercept only models as well:

```{r eval = FALSE}
y ~ 1
```

---

## Adding predictors with `+`
You can add as many predictors/covariates as you want with the simple `+` operator. See:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 +
  x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24
```

There's a shortcut to use all variables in a dataset:

```{r eval = FALSE}
y ~ .
```

But to be honest, I don't think that's the best idea.

---

## Creating interaction effects with `*` and `:`
What is more interesting, indeed, is adding interaction effects in a model. As this is the same as multiplying predictor variables, we also use the `*` sign for that.

```{r eval = FALSE}
y ~ x1 * x2
```

This creates a model formula, including both the main effects of `x1` and `x2`, and their interaction denoted by `x1:x2`. We can even be more explicit and write that in the formula directly:

```{r eval = FALSE}
y ~ x1 + x2 + x1:x2
```

Later, we will see how all of this looks when we actually feed regression models with these formulas. For the time being, it may seem to be a little bit abstract.

---

## Transforming variables with `I()`
One last point before we dive into the regression part is transforming your variables. This procedure is rather common in regression analysis. Therefore, it is also straightforward to do in `R`. For simple transformations, such as log-transformation, this can be done as shown here:

```{r eval = FALSE}
y ~ log(x)   # computes the log10 for x
y ~ scale(x) # z-transformation of x
```

If your variable is not enclosed by a specific function, you have to wrap the operation in the `I()` function. For example:

```{r eval = FALSE}
y ~ x + I(x^2) # add a quadratic term of x
```

This should show that we could also change the data type of variables within a function, e.g., by converting a numeric variable to a factor using `as.factor(x)`.

---

## Where to use formulas?
The previous description refers a lot to formulas used in regression models. Where we also use the concept of formulas with dependent variables is simple hypothesis testing as in t-tests or ANOVA.

These 'basic' statistical models can be easily used in `R`, too, as in any other statistical language. Let's try them out!

---

## Testing group differences in the distribution
A very common methods for analyzing group differences are t-tests. You can use the `t.test()` function to easily perform such a test. However, our dependent variable may not be distributed normally between groups, such as the gender of our respondents.

```{r notnormaldata, eval = FALSE}
barplot(
  table(
    gp_covid$likelihood_infection, 
    gp_covid$sex
  ),
  beside = TRUE
)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "notnormaldata", echo = FALSE}
```
]

---

## Apply a t-test
If you think about hypothesis testing, t-tests may be the first method that comes to mind. First and foremost, `R` is a statistical language. Indeed, the way to perform t-tests with `base R` is rather straightforward.

```{r}
t.test(
  likelihood_infection ~ sex,
  data = gp_covid
)
```

---

## Test of normality
Ok, t-tests are nice, but what if our data is not normally distributed in the first place, thus, violating one of the basic assumptions of performing t-tests? As we have seen in the session on *Exploratory Data Analysis*, to check this we can use a Shapiro-Wilk test of normality.

```{r}
shapiro.test(gp_covid$likelihood_infection)
```

---

## Alternative: Wilcoxon/Mann-Whitney test

If the data are not normally distributed, the Wilcoxon/Mann-Whitney test can be a suitable alternative.

```{r}
wilcox.test(
  likelihood_infection ~ sex,
  data = gp_covid
  )
```

---

## Testing multiple groups with an ANOVA
There are situations in which we want to test differences across multiple groups. The classic method to use is an analysis of variance (ANOVA) and its many variants (ANCOVA, MANOVA, etc.). Again, you can easily do that in `R`.

```{r}
nice_anova <- 
  aov(
    likelihood_infection ~ age_cat,
    data = gp_covid
  )

summary(nice_anova)
```

---

## Simple linear regressions
This may not come as a surprise at this point, but regression models are also easy to perform in `R`.

*Disclaimer*: We won't do many regression diagnostics and assume that our dependent variable is continuous for the OLS regression context here.

.tinyish[
```{r linmodel, eval = FALSE}
simple_linear_model <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

simple_linear_model
```
]

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "linmodel", echo = FALSE}
```
]

---

## Using the `summary()`function
```{r summary_function, eval = FALSE}
summary(simple_linear_model)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "summary_function", echo = FALSE}
```
]

---

## Inspecting models with `plot()`
In the visualization session, we have already learned about the built-in `base R` functions for visually inspecting the modeling of regressions in `R`.

.pull-left[
```{r lm_summary_plot, eval = FALSE}
par(mfrow = c(2, 2))
plot(simple_linear_model)
```
]

.pull-right[
```{r ref.label = "lm_summary_plot", echo = FALSE}
```
]

---

## Dummy coding is done automatically
As you can see, `R` automatically converts factors in a regression model to classic dummy-coded variables, with the reference being the first value level. Hence, there is no need to create several variables with dummy codes and add them one by one to the regression formula. 

You can inspect the contrast matrix using:

.pull-left[
```{r contrast_matrix, eval = FALSE}
contrasts(gp_covid$age_cat)
```
]

.pull-right[
```{r ref.label = "contrast_matrix", echo = FALSE}
```
]

---

## Different coding example: Effect coding
How to include a factor variable in a regression model can, of course, be changed in `R`. For a full overview, you can, e.g., have a look at this [tutorial](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables).

Let's try one alternative option out ourselves (just very briefly). I like the so-called effect coding or deviation coding, which compares the mean at a given level to the overall mean. You can create effect-coded dummies by changing the contrasts this way:

```{r}
contrasts(gp_covid$age_cat) <- contr.sum(10)
```

---

## Effect coding contrast matrix

```{r}
contrasts(gp_covid$age_cat)
```

---

## Effect coding regression
```{r effect_coded_regression, eval = FALSE}
simple_linear_model_effect_coded <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

summary(simple_linear_model_effect_coded)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "effect_coded_regression", echo = FALSE}
```
]

---

## Generalized linear regression
What we have seen so far are estimates for linear OLS regression models. A  standard `R` installation provides a multitude of other estimators/link functions, so-called family objects, e.g., binomial logistic or Poisson regression models. See `?family` for an overview.

In this session, we only show the example of logistic regression. For this  purpose, we recode our subjective risk of infection variable:

.tinyish[
```{r}
gp_covid <-
  gp_covid %>% 
  dplyr::mutate(
    likelihood_infection_dichotomous =
      dplyr::case_when(
        likelihood_infection > 4  ~ 1,
        likelihood_infection <= 4 ~ 0
      )
  )

table(gp_covid$likelihood_infection_dichotomous)
```
]

---

## Running a standard logistic regression
```{r logistic_regression, eval = FALSE}
simple_linear_model_logistic <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "logit"),
    data = gp_covid
  )

summary(simple_linear_model_logistic)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyisher[
```{r ref.label = "logistic_regression", echo = FALSE}
```
]

---

## Changing the link function to probit
```{r probit_regression, eval = FALSE}
simple_linear_model_probit <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "probit"),
    data = gp_covid
  )

summary(simple_linear_model_probit)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyisher[
```{r ref.label = "probit_regression", echo = FALSE}
```
]

---

## Handling regression results (a note probably nobody wants to hear...)
I have to admit I am not a huge fan of running regressions, searching for 'significant' p-values, and pasting the results into a table without interpreting them substantially. We _will_ (briefly) discuss how to create nice tables later on, but we will also talk about how we can apply some other techniques to gain more substantial insights into your data.

Before we do that, we will look at how to work with a readily estimated regression object in `R`. This is meant to show some of the mechanics of what is actually happening in the background when a regression is run in `R`.

---

## Accessing model results I: base `R`
Regression results are a specific type/class of objects in `R`. You can use the `str()` function to retrieve an overview of the whole structure of the object (it's a list of different information). For starters, we may want to see what the first level of this list may provide by asking for the names of the included pieces of information:

```{r}
names(simple_linear_model)
```

---

## Getting the coefficients

```{r}
simple_linear_model$coefficients
```

---

## Getting the standard errors
`lm` objects are a little bit cumbersome to use as the information is deeply nested within the object. If you want to extract the standard errors, you may need some reverse engineering:

.tinyisher[
```{r}
summary(simple_linear_model)$coefficients[,2]
```
]

Or you just compute them by yourself:

.tinyisher[
```{r}
sqrt(diag(vcov(simple_linear_model)))
```
]

---

## Getting confidence intervals
The standard `summary()` doesn't supply confidence intervals. We can use the `confint()` command to get them. For example, for the logistic regression:

```{r}
confint(simple_linear_model_logistic)
```

---

## Compare models with an ANOVA
We can also compare models with some standard tools. For example, to examine competing models, such as our logistic and probit regression, we can apply an ANOVA.

.tinyish[
```{r}
anova(simple_linear_model_logistic, simple_linear_model_probit)
```
]

---

## There are easier ways
Addon-packages, e.g., for creating tables, usually gather such information automatically, so that we don't need to apply everything by ourselves. However, we think it makes sense to at least know there are always some ways to that yourself.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2 [style = invis];
      tab2 -> tab3 [style = invis];
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height = 100)
```

Later in this session, we will also learn about some `tidy` functions that make accessing results even more straightforward.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2;
      tab2 -> tab3;
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height =100)
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/r-intro-gesis-2020/exercises/Day4_2_Confirmatory_Analysis_Exercise_1_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/r-intro-gesis-2020/solutions/Day4_2_Confirmatory_Analysis_Exercise_1_solution.html)

---

## (Simple) model predictions
It is also quite straightforward to do simple predictions from an estimated model using the `predict()` function in `R`.

.tinyish[
.pull-left[
```{r}
hist(
  gp_covid$likelihood_infection
)
```
]

.pull-right[
```{r}
hist(
  predict(simple_linear_model)
)
```
]
]

---

## Feed `predict()` with your own data
We can feed the `predict()` function with our own data to figure out what our model actually predicts when something changes. This can provide additional insights into our models:

.tinyish[
```{r}
predictions_data <-
  data.frame(
    political_orientation = 
      rep(
        mean(gp_covid$political_orientation, na.rm = TRUE), 
        2
      ),
    sex = rep(mean(gp_covid$sex), 2),
    age_cat = as.factor(c(1, 10))
  )

predict(
  object = simple_linear_model, 
  newdata = predictions_data, 
  interval = "confidence"
  )
```
]

---

## Plotting the coefficients

.pull-left[
```{r coefplot, eval = FALSE}
library(sjPlot)

plot_model(
  simple_linear_model,
  type = "est"
)
```
]

.pull-right[
```{r ref.label = "coefplot", echo = FALSE}
```
]

---

## ...multiple models

.pull-left[
```{r coefplots, eval = FALSE}
library(sjPlot)

plot_models(
  simple_linear_model,
  simple_linear_model_logistic,
  simple_linear_model_probit
)
```
]

.pull-right[
```{r ref.label = "coefplots", echo = FALSE}
```
]

---

## It's `ggplot2`-based

.tinyish[
.pull-left[
```{r coefplotsgg, eval = FALSE}
library(sjPlot)

plot_models(
  simple_linear_model,
  simple_linear_model_logistic,
  simple_linear_model_probit
) +
  scale_color_manual(
    breaks = c(
      "likelihood_infection", 
      "likelihood_infection_dichotomous.2", 
      "likelihood_infection_dichotomous.3"
    ),
    labels = c("OLS", "Logistic", "Probit"),
    values = c("#030303", "#8C8C8C", "#BABABA")
  ) + 
  labs(color = "Model") +
  theme_bw()
```
]
]

.pull-right[
```{r ref.label = "coefplotsgg", echo = FALSE}
```
]

---

## More advanced post-estimation techniques
In an OLS context, predictions of this kind are straightforward to interpret. For non-linear models, such as in logistic regression, this is way more difficult:

```{r}
predictions <- 
  predict(
    object = simple_linear_model_logistic, 
    newdata = predictions_data
  )
```

Predictions have to be converted into probabilities:

```{r}
exp(predictions) / (1 + exp(predictions))
```

---

## Predictions made easy
.pull-left[
```{r easy_prediction, eval = FALSE}
library(sjPlot)

plot_model(
  simple_linear_model_logistic, 
  terms = "age_cat",
  type = "pred"
)
```
]

.pull-right[
```{r ref.label = "easy_prediction", echo = FALSE}
```
]

These are simple predictions by holding all other variables at their mean value. If you're an avid *Stata* user, you may be familiar with using average marginal effects. You can also use them in `R` with the `margins` package!

---

## What are marginal effects (AME)?
AME provides a similar interpretation for a one-unit change as in OLS models: the average change of the dependent variable when all other variables are held constant (at their empirical value).

.tinyish[
```{r}
margins::margins(simple_linear_model_logistic)
```
]

---

## Proof: It's the same in the OLS context

.tinyish[
```{r}
margins::margins(simple_linear_model)
```

```{r}
simple_linear_model$coefficients
```
]

---

## AME are nice for predicting and plotting results as well

```{r, echo = FALSE}
invisible(
  capture.output(
    margins::cplot(
      simple_linear_model_logistic,
      "age_cat"
    )
  )
)
```

---

## Tables of analyses
The number of packages to create tables in R is infinite - at least almost. While some provide more or less the same functionality, the usual difference is what their output format is.
- `LaTeX` tables
- `HTML` tables
- `.doc`-files
- ...
- *Excel* anyone?

`R` allows you to be very flexible here. You can gather the statistics that you want, put everything into a `data.frame`, and just use the table package of your choice.

However, this approach typically requires a lot of manual work.

---

## The `stargazer` package
To avoid the hassle of too much manual work in table creation with `R`, the [`stargazer`](https://cran.r-project.org/web/packages/stargazer/index.html) package is a popular choice. It provides `LaTeX` tables by default but can also output text, `Markdown`, and `HTML` tables, or even `.doc` documents. Here's an example:

```{r stargazer, eval = FALSE}
stargazer::stargazer(
  simple_linear_model,
  simple_linear_model_effect_coded,
  simple_linear_model_logistic,
  simple_linear_model_probit,
  type = "text"
)
```

---

```{r out.width = "50%", echo = FALSE}
knitr::include_graphics("./pics/stargazer_output.png")
```

---

## Different table styles & other toggles
`stargazer` provides different table styles which you can use to format your table in accordance with the guidelines of many journals.

You can also define which statistics should be printed, how `stargazer` should deal with standard errors, and much more.

I recommend checking out the `stargazer` package, but be aware that creating tables is now a really "hot" topic in `R`. Hence, there are many packages that provide similar and, for some cases, also additional options for creating tables (we will mention some of them in the session on `R Markdown`). For example, `stargazer` does not support modern formats of statistical results, such as the output from `broom` which we will have a look next.

---

## Alternative: `huxtable`

```{r huxtable, eval = FALSE}
library(huxtable)

huxreg(
  simple_linear_model,
  simple_linear_model_effect_coded,
  simple_linear_model_logistic,
  simple_linear_model_probit
)
```

---

## Standardizing your results: tidy models with `broom`
.pull-left[
We have already entered the area of reporting statistical results. We will have a separate session on reporting on with `R Markdown`. One thing to note at this point is that more and more developers in `R` were unsatisfied with the diverse output some of the standard regression procedures provide. The outputs may be helpful to look at, but it's not great for further processing. For this purpose, we need tables.
]

.pull-right[
```{r echo = FALSE, out.width = "70%"}
knitr::include_graphics("./pics/broom package.png")
```
]

---

## 3 functions of `broom`
`broom` provides only 3 but very powerful main functions:
- `tidy()`: creates a `tibble` from your model
- `glance()`: information about your model as `tibble` ('model fit')
- `augment()`: adds information, e.g., individual fitted values to your data

Let's check them out.

---

## `tidy()`

```{r}
broom::tidy(simple_linear_model)
```

---

## `glance()`

.tinyish[
```{r}
broom::glance(simple_linear_model)
```
]

---

## `augment()`

.tinyish[
```{r}
broom::augment(simple_linear_model)
```
]

---

## Create standardized data of your results

```{r}
my_fancy_models <-
  bind_rows(
    broom::tidy(simple_linear_model) %>% 
      dplyr::mutate(model = "OLS"), 
    broom::tidy(simple_linear_model_effect_coded) %>% 
      dplyr::mutate(model = "OLS Effect Coded"),
    broom::tidy(simple_linear_model_logistic) %>% 
      dplyr::mutate(model = "Logistic"),
    broom::tidy(simple_linear_model_probit) %>% 
      dplyr::mutate(model = "Probit")
  )
```

This may seem like an unnecessary effort if we can use packages like `stargazer`. But believe me, the more you fiddle in `R`, the more you will face situations where using such procedures does not work anymore (I think this might be true for *Stata* or *SPSS* as well). Or think about possible reviewers who want to have something added. With packages like `broom` you can also create a standardized output of other types of models, such as Latent Class Analysis (LCA) (with the package [`poLCA`](https://cran.r-project.org/web/packages/poLCA/index.html)). 

---

## Example: Coefficients Plot from tidy results

The [`dotwhisker`](https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html) package is another alternative to create a coefficient plot for multiple models from a tidy results data frame.

.pull-left[
```{r dotwhisker, eval = FALSE}
library(dotwhisker)

dwplot(my_fancy_models)
```
]

.pull-right[
```{r ref.label = "dotwhisker", echo = FALSE}
```
]

---

class: center, middle

# [Exercise](https://jobreu.github.io/r-intro-gesis-2020/exercises/Day4_2_Confirmatory_Analysis_Exercise_2_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/r-intro-gesis-2020/solutions/Day4_2_Confirmatory_Analysis_Exercise_2_solution.html)

---

# Extracurricular activities
As it is such a handy package, we recommend to explore the `broom` package a bit further. Last year, Alex Hayes, one authors of the package, [gave an excellent talk at the RStudio Conference](https://rstudio.com/resources/rstudioconf-2019/solving-the-model-representation-problem-with-broom/) that is quite worthwhile to watch.